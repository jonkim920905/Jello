{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLVF-d7-7OxH"
      },
      "source": [
        "<a href=\"http://cocl.us/pytorch_link_top?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\">\n",
        "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n",
        "</a> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIvQB12f7OxN"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HGmVquZ7OxO"
      },
      "source": [
        "<h1><h1>Pre-trained-Models with PyTorch </h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkHE9ErH7OxP"
      },
      "source": [
        "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions:\n",
        "\n",
        "<ul>\n",
        "<li>change the output layer</li>\n",
        "<li> train the model</li> \n",
        "<li>  identify  several  misclassified samples</li> \n",
        " </ul>\n",
        "You will take several screenshots of your work and share your notebook. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6CFQfL67OxQ"
      },
      "source": [
        "<h2>Table of Contents</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpr0-HI-7OxQ"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<ul>\n",
        "    <li><a href=\"https://#download_data\"> Download Data</a></li>\n",
        "    <li><a href=\"https://#auxiliary\"> Imports and Auxiliary Functions </a></li>\n",
        "    <li><a href=\"https://#data_class\"> Dataset Class</a></li>\n",
        "    <li><a href=\"https://#Question_1\">Question 1</a></li>\n",
        "    <li><a href=\"https://#Question_2\">Question 2</a></li>\n",
        "    <li><a href=\"https://#Question_3\">Question 3</a></li>\n",
        "</ul>\n",
        "<p>Estimated Time Needed: <strong>120 min</strong></p>\n",
        " </div>\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqryUasW7OxR"
      },
      "source": [
        "<h2 id=\"download_data\">Download Data</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzmjtJkO7OxT"
      },
      "source": [
        "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BWu4wJY7OxU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3a4ba3a-0459-49db-c79f-7be5f3958d8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-07 00:20:51--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2598656062 (2.4G) [application/zip]\n",
            "Saving to: ‘Positive_tensors.zip’\n",
            "\n",
            "Positive_tensors.zi 100%[===================>]   2.42G  26.0MB/s    in 95s     \n",
            "\n",
            "2022-09-07 00:22:26 (26.2 MB/s) - ‘Positive_tensors.zip’ saved [2598656062/2598656062]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeuBQ0G87OxW"
      },
      "outputs": [],
      "source": [
        "!unzip -q Positive_tensors.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "va_JcZUp7OxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ab9900-a2ee-495b-97f8-42dc874f6b7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-07 00:24:54--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2111408108 (2.0G) [application/zip]\n",
            "Saving to: ‘Negative_tensors.zip’\n",
            "\n",
            "Negative_tensors.zi 100%[===================>]   1.97G  27.8MB/s    in 67s     \n",
            "\n",
            "2022-09-07 00:26:02 (29.9 MB/s) - ‘Negative_tensors.zip’ saved [2111408108/2111408108]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
        "!unzip -q Negative_tensors.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BulQQ3vd7OxZ"
      },
      "source": [
        "We will install torchvision:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtLpfDla7OxZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b16ce028-1270-4985-b97e-41ac6c09c00e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.1+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lh495oaw7Oxa"
      },
      "source": [
        "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBv7uam17Oxa"
      },
      "source": [
        "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weWBHra37Oxb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cc44a9c-99d2-4ff1-d631-ff125b96d0c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7efd6d861bf0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# These are the libraries will be used for this lab.\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import pandas\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import torch \n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import h5py\n",
        "import os\n",
        "import glob\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj0Y7Qev7Oxc"
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pylab as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcJkwj4P7Oxc"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wozvjrmD7Oxd"
      },
      "source": [
        "<h2 id=\"data_class\">Dataset Class</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP6UefRy7Oxd"
      },
      "source": [
        "This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBPJlDod7Oxe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eb0a5c6-ae1b-4167-b0d8-61ee3198b371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "# Create your own dataset object\n",
        "\n",
        "class Dataset(Dataset):\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self,transform=None,train=True):\n",
        "        directory=\"/home/dsxuser/work\"\n",
        "        positive=\"Positive_tensors\"\n",
        "        negative='Negative_tensors'\n",
        "\n",
        "        # positive_file_path=os.path.join(directory,positive)\n",
        "        # negative_file_path=os.path.join(directory,negative)\n",
        "        # positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n",
        "        # negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n",
        "        positive_files=[os.path.join(positive,file) for file in os.listdir(positive) if file.endswith(\".pt\")]\n",
        "        negative_files=[os.path.join(negative,file) for file in os.listdir(negative) if file.endswith(\".pt\")]\n",
        "        number_of_samples=len(positive_files)+len(negative_files)\n",
        "        self.all_files=[None]*number_of_samples\n",
        "        self.all_files[::2]=positive_files\n",
        "        self.all_files[1::2]=negative_files \n",
        "        # The transform is goint to be used on image\n",
        "        self.transform = transform\n",
        "        #torch.LongTensor\n",
        "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
        "        self.Y[::2]=1\n",
        "        self.Y[1::2]=0\n",
        "        \n",
        "        if train:\n",
        "            self.all_files=self.all_files[0:30000]\n",
        "            self.Y=self.Y[0:30000]\n",
        "            self.len=len(self.all_files)\n",
        "        else:\n",
        "            self.all_files=self.all_files[30000:]\n",
        "            self.Y=self.Y[30000:]\n",
        "            self.len=len(self.all_files)     \n",
        "       \n",
        "    # Get the length\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    # Getter\n",
        "    def __getitem__(self, idx):\n",
        "               \n",
        "        image=torch.load(self.all_files[idx])\n",
        "        y=self.Y[idx]\n",
        "                  \n",
        "        # If there is any transform method, apply it onto the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, y\n",
        "    \n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVJiDoj97Oxe"
      },
      "source": [
        "We create two dataset objects, one for the training data and one for the validation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vg8r7LDN7Oxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f389d8-76dd-44be-ddc6-8738699b2a7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "train_dataset = Dataset(train=True)\n",
        "validation_dataset = Dataset(train=False)\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmNUUXE87Oxf"
      },
      "source": [
        "<h2 id=\"Question_1\">Question 1</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWnkWYNJ7Oxf"
      },
      "source": [
        "<b>Prepare a pre-trained resnet18 model :</b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt3PkALr7Oxg"
      },
      "source": [
        "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOLFugwD7Oxg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "71bdaf3ae8e74ba1b9da3d8e49f2fa61",
            "4a9cd86dffaa4b0fb575d6c9a40bfa16",
            "d57d628bb4da4e51a1335993eef387c8",
            "95588331964549ef8730b80848b9d027",
            "2702123371e948b88d66b35bcf80ede4",
            "54dcaa6d9f4841f4b16c7b418e7121a1",
            "851b5f6675984dbebf829b3241dde563",
            "7723c19f66414ef39a938e2b7195614c",
            "f25f0b3146b64fd2bb8047f569ceba95",
            "baade41e7fb641e8a385d6e3b2f0ff3c",
            "c56aae75e9554d74bd4f9bde8996786f"
          ]
        },
        "outputId": "464a00b4-cbfd-42b5-dee1-e0e8fb086a3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71bdaf3ae8e74ba1b9da3d8e49f2fa61"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Step 1: Load the pre-trained model resnet18\n",
        "\n",
        "# Type your code here\n",
        "model = models.resnet18(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCHYKHZd7Oxg"
      },
      "source": [
        "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylz9FM157Oxg"
      },
      "outputs": [],
      "source": [
        "# Step 2: Set the parameter cannot be trained for the pre-trained model\n",
        "\n",
        "\n",
        "# Type your code here\n",
        "for param in model.parameters():\n",
        "  param.requires_grad=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYBit_oQ7Oxh"
      },
      "source": [
        "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcVNU2qU7Oxh"
      },
      "source": [
        "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sECYdI7K7Oxh"
      },
      "outputs": [],
      "source": [
        "model.fc = nn.Linear(512, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQqaSVeH7Oxh"
      },
      "source": [
        "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_OdJPsg7Oxi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "961d745e-44b6-42f7-ba20-04c73903842a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmmx7EKv7Oxi"
      },
      "source": [
        "<h2 id=\"Question_2\">Question 2: Train the Model</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iIhhC7h7Oxj"
      },
      "source": [
        "In this question you will train your, model:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL_RcfYN7Oxj"
      },
      "source": [
        "<b>Step 1</b>: Create a cross entropy criterion function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQfE3RJr7Oxj"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create the loss function\n",
        "\n",
        "# Type your code here\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POi3Y5Qn7Oxk"
      },
      "source": [
        "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWFbWyOv7Oxk"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(dataset=train_dataset, batch_size=100)\n",
        "validation_loader = DataLoader(dataset=validation_dataset, batch_size=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzlStE057Oxk"
      },
      "source": [
        "<b>Step 3</b>: Use the following optimizer to minimize the loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36Nojk0c7Oxk"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nut1iY9K7Oxl"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akRt8XX-7Oxl"
      },
      "source": [
        "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYpNiW4T7Oxl"
      },
      "outputs": [],
      "source": [
        "n_epochs=1\n",
        "loss_list=[]\n",
        "accuracy_list=[]\n",
        "\n",
        "correct=0\n",
        "N_test=len(validation_dataset)\n",
        "N_train=len(train_dataset)\n",
        "start_time = time.time()\n",
        "#n_epochs\n",
        "\n",
        "Loss=0\n",
        "start_time = time.time()\n",
        "for epoch in range(n_epochs):\n",
        "    for x, y in train_loader:\n",
        "        \n",
        "        model.train() \n",
        "        #clear gradient \n",
        "        optimizer.zero_grad()\n",
        "        #make a prediction \n",
        "        z = model(x)\n",
        "        # calculate loss \n",
        "        loss = criterion(z, y)\n",
        "        # calculate gradients of parameters \n",
        "        loss.backward()\n",
        "        # update parameters \n",
        "        optimizer.step()\n",
        "        loss_list.append(loss.data)\n",
        "    \n",
        "    correct=0\n",
        "    for x_test, y_test in validation_loader:\n",
        "        # set model to eval \n",
        "        model.eval()\n",
        "        #make a prediction \n",
        "        z = model(x_test)\n",
        "        #find max \n",
        "        _, yhat = torch.max(z.data, 1)\n",
        "        correct += (yhat==y_test).sum().item()\n",
        "        #Calculate misclassified  samples in mini-batch \n",
        "        #hint +=(yhat==y_test).sum().item()\n",
        "        \n",
        "        \n",
        "        \n",
        "    accuracy=correct/N_test\n",
        "    accuracy_list.append(accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h26jTM7c7Oxm"
      },
      "source": [
        "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIIQ_YYm7Oxm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b94872f8-05e8-46ce-9ae2-4c60561368fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9949"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIdSbne07Oxm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "cd286295-6fb5-4f5d-d98c-de62891087c5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(loss_list)\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4Cex9a97Oxm"
      },
      "source": [
        "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4xeTIin7Oxn"
      },
      "source": [
        "<b>Identify the first four misclassified samples using the validation data:</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YhRkVmP7Oxn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d02d4bc-5deb-4b7c-db19-cebddb34cc19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample : 38; Expected Label: tensor([1]); Obtained Label: tensor([0])\n",
            "Sample : 110; Expected Label: tensor([1]); Obtained Label: tensor([0])\n",
            "Sample : 294; Expected Label: tensor([1]); Obtained Label: tensor([0])\n",
            "Sample : 441; Expected Label: tensor([0]); Obtained Label: tensor([1])\n"
          ]
        }
      ],
      "source": [
        "validation_test = DataLoader(dataset=validation_dataset, batch_size=1)\n",
        "i = 0\n",
        "count = 0\n",
        "for x_img, y_target in validation_test:\n",
        "  model.eval()\n",
        "  #make a prediction \n",
        "  z = model(x_img)\n",
        "  #find max \n",
        "  _, yhat = torch.max(z.data, 1)\n",
        "  \n",
        "  \n",
        "  if yhat.item()!=y_target.item():\n",
        "    print(\"Sample : {}; Expected Label: {}; Obtained Label: {}\".format(str(i), str(y_target), str(yhat)))\n",
        "    count += 1\n",
        "    if count >= 4:\n",
        "      break\n",
        "    \n",
        "  i += 1\n",
        "  \n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPq35_X27Oxn"
      },
      "source": [
        "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\"> CLICK HERE </a> Click here to see how to share your notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kHY1VHK7Oxn"
      },
      "source": [
        "<h2>About the Authors:</h2> \n",
        "\n",
        "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHkfRwii7Oxo"
      },
      "source": [
        "## Change Log\n",
        "\n",
        "| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n",
        "| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n",
        "| 2020-09-21        | 2.0     | Shubham    | Migrated Lab to Markdown and added to course repo in GitLab |\n",
        "\n",
        "<hr>\n",
        "\n",
        "## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yReeSfY47Oxo"
      },
      "source": [
        "Copyright © 2018 <a href=\"https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\">MIT License</a>.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "71bdaf3ae8e74ba1b9da3d8e49f2fa61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a9cd86dffaa4b0fb575d6c9a40bfa16",
              "IPY_MODEL_d57d628bb4da4e51a1335993eef387c8",
              "IPY_MODEL_95588331964549ef8730b80848b9d027"
            ],
            "layout": "IPY_MODEL_2702123371e948b88d66b35bcf80ede4"
          }
        },
        "4a9cd86dffaa4b0fb575d6c9a40bfa16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54dcaa6d9f4841f4b16c7b418e7121a1",
            "placeholder": "​",
            "style": "IPY_MODEL_851b5f6675984dbebf829b3241dde563",
            "value": "100%"
          }
        },
        "d57d628bb4da4e51a1335993eef387c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7723c19f66414ef39a938e2b7195614c",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f25f0b3146b64fd2bb8047f569ceba95",
            "value": 46830571
          }
        },
        "95588331964549ef8730b80848b9d027": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_baade41e7fb641e8a385d6e3b2f0ff3c",
            "placeholder": "​",
            "style": "IPY_MODEL_c56aae75e9554d74bd4f9bde8996786f",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 52.5MB/s]"
          }
        },
        "2702123371e948b88d66b35bcf80ede4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54dcaa6d9f4841f4b16c7b418e7121a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "851b5f6675984dbebf829b3241dde563": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7723c19f66414ef39a938e2b7195614c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f25f0b3146b64fd2bb8047f569ceba95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "baade41e7fb641e8a385d6e3b2f0ff3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c56aae75e9554d74bd4f9bde8996786f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}